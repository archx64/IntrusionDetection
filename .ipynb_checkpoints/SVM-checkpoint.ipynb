{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "col_names = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
    "             'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n",
    "             'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root',\n",
    "             'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds',\n",
    "             'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "             'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
    "             'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "             'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "             'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "             'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label']\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('max_rows', None)\n",
    "\n",
    "df = pd.read_csv('KDDTrain.csv', header=None, names=col_names)\n",
    "df_test = pd.read_csv('KDDTest.csv', header=None, names=col_names)\n",
    "\n",
    "print('Dimensions of training set:', df.shape)\n",
    "print('Dimensions of test set:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set label distribution')\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "print('\\nTest set label distribution')\n",
    "print(df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Categorical features in training set:')\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df[col_name].unique())\n",
    "        print('Feature {col_name} has {unique_cat} categories'.format(col_name=col_name, unique_cat=unique_cat))\n",
    "        \n",
    "print('\\nDistribution of categories in service:')\n",
    "print(df['service'].value_counts().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('categorical features in test set:')\n",
    "for col_name in df_test.columns:\n",
    "    if df_test[col_name].dtypes == 'object':\n",
    "        unique_cat = len(df_test[col_name].unique())\n",
    "        print('Feature {col_name} has {unique_cat} categories'.format(col_name=col_name, unique_cat=unique_cat))\n",
    "        \n",
    "print('\\nDistribution of categories in service:')\n",
    "print(df_test['service'].value_counts().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "categorical_columns=['protocol_type', 'service', 'flag']\n",
    "df_categorical_values = df[categorical_columns]\n",
    "testdf_categorical_values = df_test[categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf_categorical_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#protocol_type\n",
    "unique_protocol = sorted(df.protocol_type.unique())\n",
    "string1 = 'Protocol_type_'\n",
    "unique_protocol2 = [string1 + x for x in unique_protocol]\n",
    "\n",
    "# service\n",
    "unique_service = sorted(df.service.unique())\n",
    "string2 = 'service_'\n",
    "unique_service2 = [string2 + x for x in unique_service]\n",
    "\n",
    "# flag\n",
    "unique_flag = sorted(df.flag.unique())\n",
    "string3 = 'flag_'\n",
    "unique_flag2 = [string3 + x for x in unique_flag]\n",
    "\n",
    "# put together\n",
    "dumcols = unique_protocol2 + unique_service2 + unique_flag2\n",
    "print(dumcols)\n",
    "\n",
    "unique_service_test = sorted(df_test.service.unique())\n",
    "unique_service2_test = [string2 + x for x in unique_service_test]\n",
    "testdumcols = unique_protocol2 + unique_service2_test + unique_flag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical_values_enc = df_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "df_categorical_values_enc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "testdf_categorical_values_enc = testdf_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "testdf_categorical_values_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "df_categorical_values_encenc = enc.fit_transform(df_categorical_values_enc)\n",
    "testdf_categorical_values_encenc = enc.fit_transform(testdf_categorical_values_enc)\n",
    "# test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_data = pd.DataFrame(df_categorical_values_encenc.toarray(), columns=dumcols)\n",
    "df_cat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf_cat_data = pd.DataFrame(testdf_categorical_values_encenc.toarray(), columns=testdumcols)\n",
    "testdf_cat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainservice = df['service'].tolist()\n",
    "testservice = df_test['service'].tolist()\n",
    "difference = list(set(trainservice) - set(testservice))\n",
    "string = 'service_'\n",
    "difference = [string + x for x in difference]\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf_cat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in difference:\n",
    "    testdf_cat_data[col] = 0\n",
    "\n",
    "testdf_cat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df.join(df_cat_data)\n",
    "newdf.drop('flag', axis=1, inplace=True)\n",
    "newdf.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf.drop('service', axis=1, inplace=True)\n",
    "\n",
    "newdf_test = df_test.join(testdf_cat_data)\n",
    "newdf_test.drop('flag', axis=1, inplace=True)\n",
    "newdf_test.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf_test.drop('service', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldf = newdf['label']\n",
    "labeldf_test = newdf_test['label']\n",
    "\n",
    "newlabeldf = labeldf.replace(\n",
    "    {'normal': 0, 'neptune': 1, 'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1, 'mailbomb': 1, 'apache2': 1,\n",
    "     'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "     'ipsweep': 2, 'nmap': 2, 'portsweep': 2, 'satan': 2, 'mscan': 2, 'saint': 2\n",
    "        , 'ftp_write': 3, 'guess_passwd': 3, 'imap': 3, 'multihop': 3, 'phf': 3, 'spy': 3, 'warezclient': 3,\n",
    "     'warezmaster': 3, 'sendmail': 3, 'named': 3, 'snmpgetattack': 3, 'snmpguess': 3, 'xlock': 3, 'xsnoop': 3,\n",
    "     'httptunnel': 3,\n",
    "     'buffer_overflow': 4, 'loadmodule': 4, 'perl': 4, 'rootkit': 4, 'ps': 4, 'sqlattack': 4, 'xterm': 4})\n",
    "newlabeldf_test = labeldf_test.replace(\n",
    "    {'normal': 0, 'neptune': 1, 'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1, 'mailbomb': 1, 'apache2': 1,\n",
    "     'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "     'ipsweep': 2, 'nmap': 2, 'portsweep': 2, 'satan': 2, 'mscan': 2, 'saint': 2\n",
    "        , 'ftp_write': 3, 'guess_passwd': 3, 'imap': 3, 'multihop': 3, 'phf': 3, 'spy': 3, 'warezclient': 3,\n",
    "     'warezmaster': 3, 'sendmail': 3, 'named': 3, 'snmpgetattack': 3, 'snmpguess': 3, 'xlock': 3, 'xsnoop': 3,\n",
    "     'httptunnel': 3,\n",
    "     'buffer_overflow': 4, 'loadmodule': 4, 'perl': 4, 'rootkit': 4, 'ps': 4, 'sqlattack': 4, 'xterm': 4})\n",
    "\n",
    "newdf['label'] = newlabeldf\n",
    "newdf_test['label'] = newlabeldf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['label'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf_test['label'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_DoS = [2, 3, 4]\n",
    "to_drop_Probe = [1, 3, 4]\n",
    "to_drop_R2L = [1, 2, 4]\n",
    "to_drop_U2R = [1, 2, 3]\n",
    "DoS_df = newdf[~newdf['label'].isin(to_drop_DoS)]\n",
    "Probe_df = newdf[~newdf['label'].isin(to_drop_Probe)]\n",
    "R2L_df = newdf[~newdf['label'].isin(to_drop_R2L)]\n",
    "U2R_df = newdf[~newdf['label'].isin(to_drop_U2R)]\n",
    "\n",
    "print('Train:')\n",
    "print('Dimensions of DoS:', DoS_df.shape)\n",
    "print('Dimensions of Probe:', Probe_df.shape)\n",
    "print('Dimensions of R2L:', R2L_df.shape)\n",
    "print('Dimensions of U2R:', U2R_df.shape)\n",
    "\n",
    "\n",
    "DoS_df_test = newdf_test[~newdf_test['label'].isin(to_drop_DoS)]\n",
    "Probe_df_test = newdf_test[~newdf_test['label'].isin(to_drop_Probe)]\n",
    "R2L_df_test = newdf_test[~newdf_test['label'].isin(to_drop_R2L)]\n",
    "U2R_df_test = newdf_test[~newdf_test['label'].isin(to_drop_U2R)]\n",
    "\n",
    "print('Test:')\n",
    "print('Dimensions of DoS:', DoS_df_test.shape)\n",
    "print('Dimensions of Probe:', Probe_df_test.shape)\n",
    "print('Dimensions of R2L:', R2L_df_test.shape)\n",
    "print('Dimensions of U2R:', U2R_df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature_Scaling\n",
    "\n",
    "X_DoS = DoS_df.drop('label', 1)\n",
    "Y_DoS = DoS_df.label\n",
    "X_Probe = Probe_df.drop('label', 1)\n",
    "Y_Probe = Probe_df.label\n",
    "X_R2L = R2L_df.drop('label', 1)\n",
    "Y_R2L = R2L_df.label\n",
    "X_U2R = U2R_df.drop('label', 1)\n",
    "Y_U2R = U2R_df.label\n",
    "\n",
    "X_DoS_test = DoS_df_test.drop('label', 1)\n",
    "Y_DoS_test = DoS_df_test.label\n",
    "X_Probe_test = Probe_df_test.drop('label', 1)\n",
    "Y_Probe_test = Probe_df_test.label\n",
    "X_R2L_test = R2L_df_test.drop('label', 1)\n",
    "Y_R2L_test = R2L_df_test.label\n",
    "X_U2R_test = U2R_df_test.drop('label', 1)\n",
    "Y_U2R_test = U2R_df_test.label\n",
    "\n",
    "colNames = list(X_DoS)\n",
    "colNames_test = list(X_DoS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler1 = preprocessing.StandardScaler().fit(X_DoS)\n",
    "X_DoS = scaler1.transform(X_DoS)\n",
    "scaler2 = preprocessing.StandardScaler().fit(X_Probe)\n",
    "X_Probe = scaler2.transform(X_Probe)\n",
    "scaler3 = preprocessing.StandardScaler().fit(X_R2L)\n",
    "X_R2L = scaler3.transform(X_R2L)\n",
    "scaler4 = preprocessing.StandardScaler().fit(X_U2R)\n",
    "X_U2R = scaler4.transform(X_U2R)\n",
    "\n",
    "scaler5 = preprocessing.StandardScaler().fit(X_DoS_test)\n",
    "X_DoS_test = scaler5.transform(X_DoS_test)\n",
    "scaler6 = preprocessing.StandardScaler().fit(X_Probe_test)\n",
    "X_Probe_test = scaler6.transform(X_Probe_test)\n",
    "scaler7 = preprocessing.StandardScaler().fit(X_R2L_test)\n",
    "X_R2L_test = scaler7.transform(X_R2L_test)\n",
    "scaler8 = preprocessing.StandardScaler().fit(X_U2R_test)\n",
    "X_U2R_test = scaler8.transform(X_U2R_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DoS.std(axis=0) #Check standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Probe.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_R2L.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_U2R.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Feature Selection\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "    \n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "np.seterr(divide='ignore', invalid='ignore');\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "X_newDoS = selector.fit_transform(X_DoS, Y_DoS)\n",
    "print(X_newDoS.shape)\n",
    "\n",
    "true = selector.get_support()\n",
    "newcolindex_DoS = [i for i, x in enumerate(true) if x]\n",
    "newcolname_DoS = list(colNames[i] for i in newcolindex_DoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_newProbe = selector.fit_transform(X_Probe,Y_Probe)\n",
    "print(X_newProbe.shape)\n",
    "newcolindex_Probe = [i for i, x in enumerate(true) if x]\n",
    "newcolname_Probe = list(colNames[i] for i in newcolindex_Probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_newR2L = selector.fit_transform(X_R2L,Y_R2L)\n",
    "print(X_newR2L.shape)\n",
    "\n",
    "newcolindex_R2L=[i for i, x in enumerate(true) if x]\n",
    "newcolname_R2L=list( colNames[i] for i in newcolindex_R2L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_newU2R = selector.fit_transform(X_U2R,Y_U2R)\n",
    "print(X_newU2R.shape)\n",
    "\n",
    "true = selector.get_support()\n",
    "newcolindex_U2R = [i for i, x in enumerate(true) if x]\n",
    "newcolname_U2R = list( colNames[i] for i in newcolindex_U2R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Univariate Features selected for DoS:')\n",
    "newcolname_DoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Univariate Features selected for Probe:')\n",
    "newcolname_Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Univariate Features selected for R2L:')\n",
    "newcolname_R2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Univariate Features selected for U2R:')\n",
    "newcolname_U2R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination, Get Importance\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "rfe = RFE(clf, n_features_to_select=1)\n",
    "rfe.fit(X_newDoS, Y_DoS)\n",
    "print ('DoS Features sorted by their rank: ')\n",
    "sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_DoS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(X_newProbe, Y_Probe)\n",
    "print ('Probe Features sorted by their rank:')\n",
    "sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_Probe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(X_newR2L, Y_R2L)\n",
    " \n",
    "print ('R2L Features sorted by their rank:')\n",
    "sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_R2L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(X_newU2R, Y_U2R)\n",
    " \n",
    "print ('U2R Features sorted by their rank:')\n",
    "sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_U2R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination, select 13 features each of 122\n",
    "\n",
    "rfe = RFE(estimator=clf, n_features_to_select=13, step=1)\n",
    "rfe.fit(X_DoS, Y_DoS)\n",
    "X_rfeDoS = rfe.transform(X_DoS)\n",
    "true = rfe.support_\n",
    "rfecolindex_DoS = [i for i, x in enumerate(true) if x]\n",
    "rfecolname_DoS = list(colNames[i] for i in rfecolindex_DoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(X_Probe, Y_Probe)\n",
    "X_rfeProbe = rfe.transform(X_Probe)\n",
    "rfecolindex_Probe = [i for i, x in enumerate(true) if x]\n",
    "rfecolname_Probe = list(colNames[i] for i in rfecolindex_Probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(X_R2L, Y_R2L)\n",
    "X_rfeR2L = rfe.transform(X_R2L)\n",
    "rfecolindex_R2L = [i for i, x in enumerate(true) if x]\n",
    "rfecolname_R2L = list(colNames[i] for i in rfecolindex_R2L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(X_U2R, Y_U2R)\n",
    "X_rfeU2R=rfe.transform(X_U2R)\n",
    "rfecolindex_U2R = [i for i, x in enumerate(true) if x]\n",
    "rfecolname_U2R = list(colNames[i] for i in rfecolindex_U2R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features selected for DoS:')\n",
    "rfecolname_DoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features selected for Probe:')\n",
    "rfecolname_Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features selected for R2L:')\n",
    "rfecolname_R2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features selected for U2R:')\n",
    "rfecolname_U2R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import time\n",
    "start = time.time()\n",
    "clf_DoS = SVC()\n",
    "clf_DoS.fit(X_DoS, Y_DoS)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf_Probe = SVC()\n",
    "clf_Probe.fit(X_Probe, Y_Probe)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf_R2L = SVC()\n",
    "clf_R2L.fit(X_R2L, Y_R2L)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf_U2R = SVC()\n",
    "clf_U2R.fit(X_U2R, Y_U2R)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf_rfeDoS = SVC()\n",
    "clf_rfeDoS.fit(X_rfeDoS, Y_DoS)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf_rfeProbe = SVC()\n",
    "clf_rfeProbe.fit(X_rfeProbe, Y_Probe)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf_rfeR2L = SVC()\n",
    "clf_rfeR2L.fit(X_rfeR2L, Y_R2L)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf_rfeU2R = SVC()\n",
    "clf_rfeU2R.fit(X_rfeU2R, Y_U2R)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Y_DoS_pred = clf_DoS.predict(X_DoS_test)\n",
    "end = time.time() - start\n",
    "print(end)\n",
    "pd.crosstab(Y_DoS_test, Y_DoS_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Y_Probe_pred = clf_Probe.predict(X_Probe_test)\n",
    "end = time.time() - start\n",
    "print(end)\n",
    "pd.crosstab(Y_Probe_test, Y_Probe_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Y_R2L_pred = clf_R2L.predict(X_R2L_test)\n",
    "end = time.time() - start\n",
    "print(end)\n",
    "pd.crosstab(Y_R2L_test, Y_R2L_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Y_U2R_pred = clf_U2R.predict(X_U2R_test)\n",
    "end = time.time() - start\n",
    "print(end)\n",
    "pd.crosstab(Y_U2R_test, Y_U2R_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "accuracy = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='accuracy')\n",
    "print('Accuracy DoS: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='precision')\n",
    "print('Precision DoS: %0.5f (+/- %0.5f)' % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='recall')\n",
    "print('Recall DoS: %0.5f (+/- %0.5f)' % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='f1')\n",
    "print('F-measure DoS: %0.5f (+/- %0.5f)' % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='accuracy')\n",
    "print('Accuracy Probe: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='precision_macro')\n",
    "print('Precision Probe: %0.5f (+/- %0.5f)' % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='recall_macro')\n",
    "print('Recall Probe: %0.5f (+/- %0.5f)' % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='f1_macro')\n",
    "print('F-measure Probe: %0.5f (+/- %0.5f)' % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='accuracy')\n",
    "print('Accuracy R2L: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='precision_macro')\n",
    "print('Precision R2L: %0.5f (+/- %0.5f)' % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='recall_macro')\n",
    "print('Recall R2L: %0.5f (+/- %0.5f)' % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='f1_macro')\n",
    "print('F-measure R2L: %0.5f (+/- %0.5f)' % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='accuracy')\n",
    "print('Accuracy U2R: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='precision_macro')\n",
    "print('Precision U2R: %0.5f (+/- %0.5f)' % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='recall_macro')\n",
    "print('Recall U2R: %0.5f (+/- %0.5f)' % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='f1_macro')\n",
    "print('F-measure U2R: %0.5f (+/- %0.5f)' % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# rfecv_DoS = RFECV(estimator=clf_DoS, step=1, cv=10, scoring='accuracy')\n",
    "# rfecv_DoS.fit(X_DoS_test, Y_DoS_test)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.xlabel('Number of features selected')\n",
    "# plt.ylabel('Cross validation score')\n",
    "# plt.title('RFECV DoS')\n",
    "# plt.plot(range(1, len(rfecv_DoS.grid_scores_) + 1), rfecv_DoS.grid_scores_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfecv_Probe = RFECV(estimator=clf_Probe, step=1, cv=10, scoring='accuracy')\n",
    "# rfecv_Probe.fit(X_Probe_test, Y_Probe_test)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.xlabel('Number of features selected')\n",
    "# plt.ylabel('Cross validation score')\n",
    "# plt.title('RFECV Probe')\n",
    "# plt.plot(range(1, len(rfecv_Probe.grid_scores_) + 1), rfecv_Probe.grid_scores_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfecv_R2L = RFECV(estimator=clf_R2L, step=1, cv=10, scoring='accuracy')\n",
    "# rfecv_R2L.fit(X_R2L_test, Y_R2L_test)\n",
    "# # Plot number of features VS. cross-validation scores\n",
    "# plt.figure()\n",
    "# plt.xlabel('Number of features selected')\n",
    "# plt.ylabel('Cross validation score')\n",
    "# plt.title('RFECV R2L')\n",
    "# plt.plot(range(1, len(rfecv_R2L.grid_scores_) + 1), rfecv_R2L.grid_scores_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfecv_U2R = RFECV(estimator=clf_U2R, step=1, cv=10, scoring='accuracy')\n",
    "# rfecv_U2R.fit(X_U2R_test, Y_U2R_test)\n",
    "# plt.figure()\n",
    "# plt.xlabel('Number of features selected')\n",
    "# plt.ylabel('Cross validation score')\n",
    "# plt.title('RFECV U2R')\n",
    "# plt.plot(range(1, len(rfecv_U2R.grid_scores_) + 1), rfecv_U2R.grid_scores_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DoS_test2 = X_DoS_test[:,rfecolindex_DoS]\n",
    "X_Probe_test2 = X_Probe_test[:,rfecolindex_Probe]\n",
    "X_R2L_test2 = X_R2L_test[:,rfecolindex_R2L]\n",
    "X_U2R_test2 = X_U2R_test[:,rfecolindex_U2R]\n",
    "X_U2R_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Y_DoS_pred2 = clf_rfeDoS.predict(X_DoS_test2)\n",
    "end = time.time() - start\n",
    "print(end)\n",
    "pd.crosstab(Y_DoS_test, Y_DoS_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Y_Probe_pred2 = clf_rfeProbe.predict(X_Probe_test2)\n",
    "end = time.time() - start\n",
    "print(end)\n",
    "pd.crosstab(Y_Probe_test, Y_Probe_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Y_R2L_pred2 = clf_rfeR2L.predict(X_R2L_test2)\n",
    "end = time.time() - start\n",
    "print(end)\n",
    "pd.crosstab(Y_R2L_test, Y_R2L_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Y_U2R_pred2 = clf_rfeU2R.predict(X_U2R_test2)\n",
    "end = time.time() - start\n",
    "print(end)\n",
    "pd.crosstab(Y_U2R_test, Y_U2R_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='accuracy')\n",
    "print('Accuracy Dos Rfec: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='precision')\n",
    "print('Precision Dos Rfec: %0.5f (+/- %0.5f)' % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='recall')\n",
    "print('Recall Dos Rfec: %0.5f (+/- %0.5f)' % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='f1')\n",
    "print('F-measure Dos Rfec: %0.5f (+/- %0.5f)' % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='accuracy')\n",
    "print('Accuracy Probe Rfec: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='precision_macro')\n",
    "print('Precision Probe Rfec: %0.5f (+/- %0.5f)' % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='recall_macro')\n",
    "print('Recall Probe Rfec: %0.5f (+/- %0.5f)' % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='f1_macro')\n",
    "print('F-measure Probe Rfec: %0.5f (+/- %0.5f)' % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='accuracy')\n",
    "print('Accuracy R2L Rfec: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='precision_macro')\n",
    "print('Precision R2L Rfec: %0.5f (+/- %0.5f)' % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='recall_macro')\n",
    "print('Recall R2L Rfec: %0.5f (+/- %0.5f)' % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='f1_macro')\n",
    "print('F-measure R2L Rfec: %0.5f (+/- %0.5f)' % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='accuracy')\n",
    "print('Accuracy U2R Rfec: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='precision_macro')\n",
    "print('Precision U2R Rfec: %0.5f (+/- %0.5f)' % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='recall_macro')\n",
    "print('Recall U2R Rfec: %0.5f (+/- %0.5f)' % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='f1_macro')\n",
    "print('F-measure U2R Rfec: %0.5f (+/- %0.5f)' % (f.mean(), f.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strafified Cross Validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "print('Accuracy DoS: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "print('Accuracy Probe: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "print('Accuracy R2L: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "print('Accuracy U2R: %0.5f (+/- %0.5f)' % (accuracy.mean(), accuracy.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation 2, 5, 10, 30, 50 FOLD\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=2, scoring='accuracy')\n",
    "print(\"Accuracy DoS 2 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy DoS 5 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy DoS 10 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=2, scoring='accuracy')\n",
    "print(\"Accuracy Probe 2 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "\n",
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy Probe 5 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "\n",
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy Probe 10 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=2, scoring='accuracy')\n",
    "print(\"Accuracy R2L 2 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "\n",
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy R2L 5 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "\n",
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy R2L 10 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=2, scoring='accuracy')\n",
    "print(\"Accuracy U2R 2 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "\n",
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy U2R 5 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "\n",
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy U2R 10 FOLD: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
